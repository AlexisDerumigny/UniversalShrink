% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/ridge_higher_order_shrinkage.R
\name{ridge_higher_order_shrinkage}
\alias{ridge_higher_order_shrinkage}
\title{Ridge higher order shrinkage}
\usage{
ridge_higher_order_shrinkage(
  X,
  m = 3,
  centeredCov = TRUE,
  t = NULL,
  interval = c(0, 50),
  verbose = 0,
  inversionMethod = "solve"
)
}
\arguments{
\item{X}{data matrix (rows are observations, columns are features).}

\item{m}{order of the shrinkage. Should be at least 1.}

\item{centeredCov}{Boolean variable. If \code{TRUE}, the covariance matrix is
computed using centering (i.e. in the general case where the mean of the
random vector may be non-zero).
If \code{FALSE} the covariance matrix is computed assuming that the mean of
the random vector of interest is \code{0} (i.e. does not need to be estimated).}

\item{t, interval}{\code{t} is the penalization parameter, and \code{interval}
is the interval over which the loss is optimized over (with respect to \code{t}).}

\item{verbose}{a number indicating whether to print intermediary values and
details about the progress of the computations. A value of \code{0} indicates
no printing at all, while higher values indicate increasingly more detailed
(more verbose) output.}

\item{inversionMethod}{method for inverting the matrix \eqn{M}. Current
possible choices are \code{solve} (usual inverse) and \code{ginv}
(Moore-Penrose generalize inverse via \code{MASS::\link[MASS]{ginv}}).}
}
\description{
The findings of Bodnar and Parolya (2025) yield the ridge higher-order nonlinear
shrinkage estimator of the precision matrix given by
\deqn{
\mathbf{S}_{HOS}^-(t^*)
=
\hat{\alpha}_0^-(t^*)\mathbf{I}_p
+
\sum_{j=1}^m
\hat{\alpha}_j^-(t^*)
(\mathbf{S}_n^-(t))^j
}
where \eqn{\mathbf{S}_n^-(t^*)=(\mathbf{S}_n+t^*\mathbf{I}_p)^{-1}} is the Ridge
estimator evaluated at optimal point \eqn{t^*>0} and
\eqn{\mathbf{S}} is the sample covariance matrix and \eqn{\mathbf{I}_p} is
\eqn{p}-dimensional identity matrix. The vector \eqn{
\hat{\boldsymbol{\alpha}}^{-}(m,t^*)
=
(\hat{\alpha}_0^-(t^*),\hat{\alpha}_1^-(t^*),\ldots,\hat{\alpha}_m^-(t^*))^\top
}
is given by
\deqn{
\hat{\boldsymbol{\alpha}}^{-}(m,t^*)
=
\widehat{\mathbf{M}}^{-}(m,t^*)^{-1}
\hat{\mathbf{m}}^{-}(m,t^*)
.}
The optimal value \eqn{t^*} is obtained by minimizing over \eqn{t>0} the function
\deqn{
\hat{L}_2^{-}(m,t)
=
1
-
\hat{\mathbf{m}}^{-}(m,t)^\top
\widehat{\mathbf{M}}^{-}(m,t)^{-1}
\hat{\mathbf{m}}^{-}(m,t)
.}
Here,
\eqn{
\hat{\mathbf{m}}^{-}(m,t)
=
\begin{pmatrix}
\hat{q}_1 \\
\hat{\tilde{s}}_{1,1}(t) \\
\vdots \\
\hat{\tilde{s}}_{m,1}(t)
\end{pmatrix}
}
and
\eqn{
\widehat{\mathbf{M}}^{-}(m,t)
=
\begin{pmatrix}
\hat{q}_2 & \hat{\tilde{s}}_{1,2}(t) & \ldots & \hat{\tilde{s}}_{m,2}(t) \\
\hat{\tilde{s}}_{1,2}(t) & \hat{\tilde{s}}_2(t) & \ldots & \hat{\tilde{s}}_{m+1,2}(t) \\
\vdots & \vdots & \ddots & \vdots \\
\hat{\tilde{s}}_{m,2}(t) & \hat{\tilde{s}}_{m+1,2}(t) & \ldots & \hat{\tilde{s}}_{2m,2}(t)
\end{pmatrix}
}, where
the quantities \eqn{\hat{\tilde{s}}_{j,l}(t)} are defined in
Bodnar and Parolya (2025). This procedure ensures that the loss
\eqn{||\mathbf{S}_{HOS}^-(t^*)\boldsymbol{\Sigma}-\mathbf{I}_p||^2_F}
is asymptotically minimized with probability one.
}
\examples{

n = 10
p = 2 * n
mu = rep(0, p)

# Generate Sigma
X0 <- MASS::mvrnorm(n = 10*p, mu = mu, Sigma = diag(p))
H <- eigen(t(X0) \%*\% X0)$vectors
Sigma = H \%*\% diag(seq(1, 0.02, length.out = p)) \%*\% t(H)

# Generate example dataset
X <- MASS::mvrnorm(n = n, mu = mu, Sigma=Sigma)

precision_MoorePenrose_Cent = Moore_Penrose_target(X, centeredCov = TRUE)
precision_MoorePenrose_NoCent = Moore_Penrose_target(X, centeredCov = FALSE)

FrobeniusLoss2(precision_MoorePenrose_Cent, Sigma = Sigma)
FrobeniusLoss2(precision_MoorePenrose_NoCent, Sigma = Sigma)

for (m in 1:2){
  cat("m = ", m, "\n")
  precision_higher_order_shrinkage_Cent = 
      ridge_higher_order_shrinkage(X, m = m, centeredCov = TRUE, t = 10)
      
  precision_higher_order_shrinkage_NoCent = 
      ridge_higher_order_shrinkage(X, m = m, centeredCov = FALSE, t = 10)
      
  print(FrobeniusLoss2(precision_higher_order_shrinkage_Cent, Sigma = Sigma))
  
  print(FrobeniusLoss2(precision_higher_order_shrinkage_NoCent, Sigma = Sigma))
}


precision_higher_order_shrinkage_Cent = 
  ridge_higher_order_shrinkage(X, m = 1, centeredCov = TRUE, t = 100)


precision_higher_order_shrinkage_Cent$alpha
precision_higher_order_shrinkage_Cent$M
precision_higher_order_shrinkage_Cent$hm

FrobeniusLoss2(precision_higher_order_shrinkage_Cent, Sigma = Sigma)


# For comparison TODO: move this as unit test

precision_target = ridge_target(X, centeredCov = TRUE, t = 100)
FrobeniusLoss2(precision_target, Sigma = Sigma)


# Examples for ridge_higher_order_shrinkage where the optimization is done 
# with respect to t

for (m in 1:2){
  cat("m = ", m, "\n")
  precision_higher_order_shrinkage_Cent = 
      ridge_higher_order_shrinkage(X, m = m, centeredCov = TRUE)
      
  precision_higher_order_shrinkage_NoCent = 
      ridge_higher_order_shrinkage(X, m = m, centeredCov = FALSE)
      
  print(FrobeniusLoss2(precision_higher_order_shrinkage_Cent, Sigma = Sigma))
  
  print(FrobeniusLoss2(precision_higher_order_shrinkage_NoCent, Sigma = Sigma))
}

precision_higher_order_shrinkage_Cent = 
      ridge_higher_order_shrinkage(X, m = 1, centeredCov = TRUE)

precision_ridge_target_Cent = ridge_target(X, centeredCov = TRUE)
  
FrobeniusLoss2(precision_higher_order_shrinkage_Cent, Sigma = Sigma)
FrobeniusLoss2(precision_ridge_target_Cent, Sigma = Sigma)



}
