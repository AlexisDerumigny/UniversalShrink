% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/Moore_Penrose_target.R
\name{Moore_Penrose_target}
\alias{Moore_Penrose_target}
\title{First-order shrinkage of the Moore-Penrose inverse towards a fixed target}
\usage{
Moore_Penrose_target(X, centeredCov = TRUE, Pi0 = NULL, verbose = 0)
}
\arguments{
\item{X}{data matrix (rows are observations, columns are features).}

\item{centeredCov}{Boolean variable. If \code{TRUE}, the covariance matrix is
computed using centering (i.e. in the general case where the mean of the
random vector may be non-zero).
If \code{FALSE} the covariance matrix is computed assuming that the mean of
the random vector of interest is \code{0} (i.e. does not need to be estimated).}

\item{Pi0}{prior of the precision matrix. This a \code{p} by \code{p} matrix, used as
a target for the shrinkage. Default value is the identity matrix of size \code{p}.
As an advice, it should be a symmetric positive-definite matrix, but this is
not checked for.}

\item{verbose}{a number indicating whether to print intermediary values and
details about the progress of the computations. A value of \code{0} indicates
no printing at all, while higher values indicate increasingly more detailed
(more verbose) output.}
}
\value{
the estimator of the precision matrix
(a \code{p} by \code{p} matrix, i.e. the inverse of the covariance matrix).
}
\description{
Following Bodnar and Parolya (2006), the shrinkage estimator
for the precision matrix using the Moore-Penrose inverse of the sample
covariance matrix \eqn{\mathbf{S}_n} is computed as
\deqn{
\widehat{\boldsymbol{\Pi}}_{MP}=\hat{\alpha}_{MP}^*\mathbf{S}_n^{+}+
\hat{\beta}_{MP}^*\boldsymbol{\Pi}_0\,,
} where \eqn{\mathbf{S}^+_n} denotes the Moore-Penrose inverse the sample
covariance matrix, \eqn{\boldsymbol{\Pi}_0} is the shrinkage target
(\eqn{\boldsymbol{\Pi}_0=\mathbf{I}_p}, i.e., identity matrix by default) and
\eqn{\hat{\alpha}_{MP}^*} and \eqn{\hat{\beta}_{MP}^*} are the optimal
shrinkage intensities (in sense of minimizing asymptotically
\eqn{||\widehat{\boldsymbol{\Pi}}_{MP}\boldsymbol{\Sigma}-\mathbf{I}_p||^2_F}) given by
\deqn{
\hat{\alpha}_{MP}^* =
 \dfrac{
   \hat{d}_1\left(\frac{1}{p}\boldsymbol{\Sigma}\right)
   \hat{q}_2\left(\frac{1}{p}\boldsymbol{\Pi}_0^2\right)
   -
     \hat{d}_1\left(0,\frac{1}{p}\boldsymbol{\Sigma}^2\boldsymbol{\Pi}_0\right)
   \hat{q}_1\left(\frac{1}{p}\boldsymbol{\Pi}_0\right)
 }{
   -\dfrac{1}{\hat{h}_2}
   \left(
     \hat{d}_2\left(\frac{1}{p}\boldsymbol{\Sigma}^2\right)
     -
       \hat{d}_1\left(\frac{1}{p}\boldsymbol{\Sigma}^2\right)
     \dfrac{\hat{h}_3}{\hat{h}_2}
     \right)
   \hat{q}_2\left(\frac{1}{p}\boldsymbol{\Pi}_0^2\right)
   -
     \dfrac{1}{\hat{h}_2}
   \hat{d}_1^2\left(0,\frac{1}{p}\boldsymbol{\Sigma}^2\boldsymbol{\Pi}_0\right)
 }
} and
\deqn{
\hat{\beta}_{MP}^* =
\dfrac{
\left(
\hat{d}_2\left(\frac{1}{p}\boldsymbol{\Sigma}^2\right)
-
\hat{d}_1\left(\frac{1}{p}\boldsymbol{\Sigma}^2\right)
\dfrac{\hat{h}_3}{\hat{h}_2}
\right)
\hat{q}_1\left(\frac{1}{p}\boldsymbol{\Pi}_0\right)
+
\hat{d}_1\left(\frac{1}{p}\boldsymbol{\Sigma}\right)
\hat{d}_1\left(0,\frac{1}{p}\boldsymbol{\Sigma}^2\boldsymbol{\Pi}_0\right)
}{
\left(
\hat{d}_2\left(\frac{1}{p}\boldsymbol{\Sigma}^2\right)
-
\hat{d}_1\left(\frac{1}{p}\boldsymbol{\Sigma}^2\right)
\dfrac{\hat{h}_3}{\hat{h}_2}
\right)
\hat{q}_2\left(\frac{1}{p}\boldsymbol{\Pi}_0^2\right)
+
\hat{d}_1^2\left(0,\frac{1}{p}\boldsymbol{\Sigma}^2\boldsymbol{\Pi}_0\right)
}\,,
}
where \eqn{\hat{d}_1\left(\frac{1}{p}\boldsymbol{\Sigma}\right)},
\eqn{\hat{d}_1\left(\frac{1}{p}\boldsymbol{\Sigma}^2\right)},
\eqn{\hat{d}_1\left(0,\frac{1}{p}\boldsymbol{\Sigma}^2\boldsymbol{\Pi}_0\right)},
\eqn{\hat{d}_2\left(\frac{1}{p}\boldsymbol{\Sigma}^2\right)} are defined in Theorem 3.2
in Bodnar and Parolya (2026), while the rest \eqn{\hat{v}(0)}, \eqn{\hat{h}_2}, \eqn{\hat{h}_3},
\eqn{\hat{d}_1(\frac{1}{p}\mathbf{I}_p)},
\eqn{\hat{d}_1(\frac{1}{p}\boldsymbol{\Pi}_0)},
\eqn{\hat{d}_2(\frac{1}{p}\mathbf{I}_p)},
\eqn{\hat{d}_0(0,\frac{1}{p}\boldsymbol{\Pi}_0)},
\eqn{\hat{q}_1(\frac{1}{p}\boldsymbol{\Pi}_0)}
and
\eqn{\hat{q}_2(\frac{1}{p}\boldsymbol{\Pi}_0^2)}
are given in the supplement to Bodnar and Parolya (2026).
}
\examples{
n = 50
p = 5 * n
mu = rep(0, p)

# Generate Sigma
X0 <- MASS::mvrnorm(n = 10*p, mu = mu, Sigma = diag(p))
H <- eigen(t(X0) \%*\% X0)$vectors
Sigma = H \%*\% diag(seq(1, 0.02, length.out = p)) \%*\% t(H)

# Generate example dataset
X <- MASS::mvrnorm(n = n, mu = mu, Sigma=Sigma)

precision_MoorePenrose_Cent = Moore_Penrose_target(X, centeredCov = TRUE)
   
precision_MoorePenrose_NoCent = Moore_Penrose_target(X, centeredCov = FALSE)

precisionTrue = solve(Sigma)

estimatedCov_NLshrink = cov_analytical_NL_shrinkage(X)
estimatedCov_QISshrink = cov_quadratic_inverse_shrinkage(X)

precision_NLshrink = solve(estimatedCov_NLshrink)
precision_QISshrink = solve(estimatedCov_QISshrink)

FrobeniusLoss2(precision_MoorePenrose_Cent, Sigma = Sigma)
FrobeniusLoss2(precision_MoorePenrose_NoCent, Sigma = Sigma)
FrobeniusLoss2(precision_NLshrink, Sigma = Sigma, type = "precision")
FrobeniusLoss2(precision_QISshrink, Sigma = Sigma, type = "precision")

# We now use the true value of the precision matrix as a target for shrinkage
precision_MoorePenrose_Cent_trueSigma = 
  Moore_Penrose_target(X, centeredCov = TRUE, Pi0 = solve(Sigma))
precision_MoorePenrose_NoCent_trueSigma = 
  Moore_Penrose_target(X, centeredCov = FALSE, Pi0 = solve(Sigma))                                                        
                                                        
FrobeniusLoss2(precision_MoorePenrose_Cent_trueSigma, Sigma = Sigma)
FrobeniusLoss2(precision_MoorePenrose_NoCent_trueSigma, Sigma = Sigma)
# this is indeed much closer than before


}
\references{
Nestor Parolya & Taras Bodnar (2026).
Reviving pseudo-inverses: Asymptotic properties of large dimensional
Moore-Penrose and Ridge-type inverses with applications.
\doi{10.48550/arXiv.2403.15792}
}
